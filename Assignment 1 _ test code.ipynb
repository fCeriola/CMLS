{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caroline-dress",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 1 - Speech recognition\n",
    "\n",
    "what I will do:\n",
    "* load all the audio files\n",
    "* extract from the filenames the digit labels (to use as GT)\n",
    "* compute all the MFCCs (to use as audio feature)\n",
    "* split the data into train and test sets\n",
    "* use a Random Forest algorithm\n",
    "* test the model\n",
    "* compute a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "higher-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa.display\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-health",
   "metadata": {},
   "source": [
    "## Create the dictionary\n",
    "\n",
    "I create a dictionary that I will use both to create the Pandas data frame (which I will use only to split train and test sets) and to store the MFCCs values before converting them to matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "biological-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'mfcc':[], 'labels':[], 'speaker':[], 'train_set':[], 'train_target_set':[], 'test_set':[], 'test_target_set':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-communist",
   "metadata": {},
   "source": [
    "## Compute the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respected-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some preliminary steps:\n",
    "\n",
    "data_path = \"recordings\"  \n",
    "n_mfcc = 20 \n",
    "elements = os.listdir(data_path)  \n",
    "\n",
    "#number_of_elements = len(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-fraud",
   "metadata": {},
   "source": [
    "### Some notes:\n",
    "\n",
    "* each audio file generates different numbers of samples --> to make our data homogeneous we'll take the mean value of all the MFCCs we're collecting from all samples. This way we get only 13 coefficients for each audio file --> 13 rows vector for each audio file\n",
    "\n",
    "\n",
    "* the MFCCs get stored in the dictionary key \"mfcc\" which is a list of 18 vectors with 13 rows, where 18 is the total number of audio files we're working with and 13 is the number of MFCCs for each of them\n",
    "\n",
    "\n",
    "* I will use tqdm to see how long it takes for the for loop to finish (took 56 sec on my computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "trying-timer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3000/3000 [01:46<00:00, 28.06it/s]\n"
     ]
    }
   ],
   "source": [
    "#extract the digit labels from each wav file\n",
    "for i, filename in enumerate(tqdm.tqdm(elements)):\n",
    "    file_name_components = filename.split(\"_\")\n",
    "    digit_label = file_name_components[0]\n",
    "    data[\"labels\"].append(digit_label)\n",
    "    #data[\"speaker\"].append(speaker_name)  #this is not really a necessary data to collect\n",
    "        \n",
    "    #load the audio files\n",
    "    audio, sr = librosa.load(os.path.join(data_path, filename))\n",
    "        \n",
    "    #compute the MFCCs for each audio file\n",
    "    mfcc = librosa.feature.mfcc(audio, n_fft=2048, hop_length=512, n_mfcc=n_mfcc)\n",
    "    feature_vector =np.mean(mfcc, axis=1)\n",
    "    \n",
    "    #features_matrix[i,:] = feature_vector\n",
    "    data[\"mfcc\"].append(feature_vector)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-theta",
   "metadata": {},
   "source": [
    "Next there are some checks that I did to see if everything was working properly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data[\"mfcc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"mfcc\"][0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"mfcc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-capability",
   "metadata": {},
   "source": [
    "### Listen to the last audio loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-chemical",
   "metadata": {},
   "source": [
    "### Visualize the last MFCCs computed (no mean value yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(mfcc,\n",
    "                        x_axis = 'time',\n",
    "                        sr=sr)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('MFCCs')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-marijuana",
   "metadata": {},
   "source": [
    "## Creating the Pandas Data Frame\n",
    "\n",
    "I create this data frame for the sole purpose of splitting the data into train and test sets in a non-biased way FOR EACH class of digits. The train_test_split method from scikit learn will provide the randomness and it doesn't require data frames per se... BUT I need the data frame to apply the method on each different class of digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adopted-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data[\"mfcc\"])   \n",
    "df['digit_label']=data[\"labels\"]\n",
    "#df['speaker_name']=data[\"speaker\"]  #not needed\n",
    "\n",
    "# call df below if you want to visualize the data frame\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-shoulder",
   "metadata": {},
   "source": [
    "## Split test and training sets\n",
    "\n",
    "NOTE : This is meant to work with whatever group of digits (may be 0-3, 4-7, 0-9...).\n",
    "\n",
    "* In the first part of the for loop I pick iteratively only one class of digits (first the 0s, then the 1s, the 2s...) and divide the data frame into data and targets.\n",
    "\n",
    "* In the second part of the loop I actually split the data and the targets into train and test so that the test data will be around 20% of the total (see the test_size argument)\n",
    "\n",
    "* In the third part I return to numpy arrays, so that I can merge together the train and test data I got from each different digit class (because the classifier works with 2d matrices).\n",
    "\n",
    "This is very fast for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stupid-vancouver",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 169.93it/s]\n"
     ]
    }
   ],
   "source": [
    "digits=np.unique(data[\"labels\"])  #understand what digits we are working with\n",
    "\n",
    "for i in tqdm.tqdm(digits):\n",
    "    df_same_digit = df[df.digit_label == i]\n",
    "    X = df_same_digit.iloc[:, 0:13]\n",
    "    y = df_same_digit.iloc[:, 13] \n",
    "    \n",
    "    #splitting \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.2)\n",
    "    \n",
    "    \n",
    "    X_train_np = (X_train).to_numpy()    #using Pandas' function to transform data frame into numpy array\n",
    "    data[\"train_set\"].append(X_train_np)\n",
    "    TRAIN = np.concatenate(data[\"train_set\"], axis=0)\n",
    "    \n",
    "    X_test_np = (X_test).to_numpy()   \n",
    "    data[\"test_set\"].append(X_test_np)\n",
    "    TEST = np.concatenate(data[\"test_set\"], axis=0)\n",
    "    \n",
    "    y_train_np = (y_train).to_numpy()    \n",
    "    data[\"train_target_set\"].append(y_train_np)\n",
    "    TRAIN_TARGET = np.concatenate(data[\"train_target_set\"], axis=0)   \n",
    "    \n",
    "    y_test_np = (y_test).to_numpy()    \n",
    "    data[\"test_target_set\"].append(y_test_np)\n",
    "    TEST_TARGET = np.concatenate(data[\"test_target_set\"], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-negotiation",
   "metadata": {},
   "source": [
    "Here I run some codes to check how my data now looks.\n",
    "\n",
    "First I take a look at the results I get immediately after the splitting. You can notice that the splitting produced pandas data frames in which is very easy to see how the splitting algorithm worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "royal-forestry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0           1           2          3          4          5   \\\n",
      "2834 -420.241547  190.577744  -48.976040  18.850777  42.124546 -32.847275   \n",
      "2845 -404.643829  185.162552  -43.370239  12.719451  35.329224 -33.192661   \n",
      "2763 -353.636444  223.851776  -30.640776  23.995687  15.994088 -49.407070   \n",
      "2993 -517.000732  232.470825  -73.659012  14.383049  22.255398 -45.507545   \n",
      "2985 -499.941162  227.532166  -65.901955   6.783215  25.549255 -38.577709   \n",
      "...          ...         ...         ...        ...        ...        ...   \n",
      "2951 -507.667114  246.612625  -82.658821   0.318944   9.632804 -52.175011   \n",
      "2892 -357.355865  224.650391  -77.109909  30.480385  39.189312 -32.906834   \n",
      "2817 -464.087219  162.275909  -29.771009  23.153200  39.437393 -22.088482   \n",
      "2747 -385.234375  188.604111 -101.337753  13.828812  34.147106 -47.880974   \n",
      "2872 -364.017548  228.201538  -78.769882  25.889086  33.843235 -33.839317   \n",
      "\n",
      "             6          7          8          9          10         11  \\\n",
      "2834  -3.410741  17.886734 -19.947641  -7.357065  11.873301  -0.841143   \n",
      "2845  -6.361543  15.791821 -20.081717 -10.518966   9.340371  -1.560475   \n",
      "2763   0.496405   3.568731 -29.181135   0.523287  -0.922073 -21.486746   \n",
      "2993  20.073349  10.186001 -36.696804   7.039240   1.434577 -20.690067   \n",
      "2985  10.423658   8.890407 -32.388992   0.818184   3.602565 -15.056807   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "2951  21.156101  10.103542 -38.241600   9.939236   5.875996 -17.549618   \n",
      "2892  16.573452  -4.419653 -32.790779  17.440086  -0.717752 -21.207094   \n",
      "2817  -2.321899  11.197315 -16.861921  -5.406791   8.033981  -1.693926   \n",
      "2747  -6.390952 -19.066439 -44.983822   5.332791 -15.657747 -39.767696   \n",
      "2872  19.156986  -1.096331 -33.315929  17.480858   1.015041 -21.555546   \n",
      "\n",
      "             12  \n",
      "2834  -2.898686  \n",
      "2845  -4.373389  \n",
      "2763  -8.956542  \n",
      "2993   7.344616  \n",
      "2985   1.772798  \n",
      "...         ...  \n",
      "2951  10.647500  \n",
      "2892   8.772868  \n",
      "2817  -3.397831  \n",
      "2747   1.308131  \n",
      "2872   7.945726  \n",
      "\n",
      "[240 rows x 13 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    " print(X_train)  #this is just for the digit \"2\"\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-product",
   "metadata": {},
   "source": [
    "Once you transform those back to numpy arrays you can get this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train_np)          #notice that it's the same as above, obviously\n",
    "# print((X_train).shape)\n",
    "# print(type(X_train_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-virus",
   "metadata": {},
   "source": [
    "It may be useful to check also the concatenated matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "informal-algebra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 13)\n"
     ]
    }
   ],
   "source": [
    " print(TRAIN.shape)\n",
    "# print(type(TRAIN))\n",
    "# print(TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a look at the target\n",
    "# print(TRAIN_TARGET.shape)\n",
    "# print(TRAIN_TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-jumping",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's recap and see if everything is ready to be used with the classifier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "controlling-directory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 13)\n",
      "(2400,)\n",
      "(600, 13)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN.shape)\n",
    "print(TRAIN_TARGET.shape)\n",
    "print(TEST.shape)\n",
    "print(TEST_TARGET.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-clearance",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "considerable-consumption",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3a05b5400902>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRAIN_TARGET\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\CMLS\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_class_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_y_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mDOUBLE\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\CMLS\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_y_class_weight\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_y_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\CMLS\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    181\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    182\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "#create the random forest classifier\n",
    "clf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "\n",
    "#training\n",
    "clf.fit(TRAIN, TRAIN_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = clf.predict(TEST)\n",
    "\n",
    "# call the following to actually see the prediction results:\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-april",
   "metadata": {},
   "source": [
    "To get the confusion matrix I use Pandas function \"crosstab\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(TEST_TARGET, a,  rownames=['Actual digits'], colnames=['Predicted digits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-renewal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
